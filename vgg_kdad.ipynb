{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kgYzMJfILnP",
        "outputId": "1f4461b1-45f2-4a48-b16b-4365eb4c9090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Knowledge_Distillation_AD'...\n",
            "remote: Enumerating objects: 137, done.\u001b[K\n",
            "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
            "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
            "remote: Total 137 (delta 52), reused 79 (delta 36), pack-reused 30\u001b[K\n",
            "Receiving objects: 100% (137/137), 19.88 MiB | 21.29 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n",
            "/content/Knowledge_Distillation_AD\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/LucStrater/Knowledge_Distillation_AD.git\n",
        "%cd /content/Knowledge_Distillation_AD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M2ekXKsmOQ4s"
      },
      "outputs": [],
      "source": [
        "config = {}\n",
        "\n",
        "# Data parameters\n",
        "config[\"experiment_name\"] = 'local_equal_net'\n",
        "config[\"dataset_name\"] = 'cifar10'\n",
        "config[\"last_checkpoint\"] = 200\n",
        "\n",
        "# Training parameters\n",
        "config[\"num_epochs\"] = 1 # put 201 if you want to train from scratch\n",
        "config[\"batch_size\"] = 64\n",
        "config[\"learning_rate\"] = 1e-3\n",
        "config[\"mvtec_img_size\"] = 128\n",
        "config[\"normal_class\"] = 3\n",
        "config[\"lamda\"] = 0.01\n",
        "config[\"pretrain\"] = True # True =use pre-trained vgg as source network --- False =use random initialize\n",
        "config[\"use_bias\"] = False # True =using bias term in neural network layer\n",
        "config[\"equal_network_size\"] = False # True =using equal network size for cloner and source network --- False =smaller network for cloner\n",
        "config[\"direction_loss_only\"] = False\n",
        "config[\"continue_train\"] = True\n",
        "\n",
        "# Test parameters\n",
        "config[\"localization_test\"] = False # True =For Localization Test --- False =For Detection\n",
        "config[\"localization_method\"] = 'gbp' # gradients , smooth_grad , gbp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nTlWHsDFIQOE"
      },
      "outputs": [],
      "source": [
        "from test import *\n",
        "from utils.utils import *\n",
        "from utils.dataloader import *\n",
        "from pathlib import Path\n",
        "from torch.autograd import Variable\n",
        "import pickle\n",
        "from utils.test_functions import detection_test\n",
        "from utils.loss_functions import *\n",
        "from models.network import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b-yyCr9jIX_o"
      },
      "outputs": [],
      "source": [
        "def train(config):\n",
        "    direction_loss_only = config[\"direction_loss_only\"]\n",
        "    normal_class = config[\"normal_class\"]\n",
        "    learning_rate = float(config['learning_rate'])\n",
        "    num_epochs = config[\"num_epochs\"]\n",
        "    lamda = config['lamda']\n",
        "    continue_train = config['continue_train']\n",
        "    last_checkpoint = config['last_checkpoint']\n",
        "\n",
        "    cwd = os.getcwd()\n",
        "    checkpoint_path = \"{}/outputs/{}/{}/checkpoints/\".format(cwd, config['experiment_name'], config['dataset_name'])\n",
        "\n",
        "    # create directory\n",
        "    Path(checkpoint_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_dataloader, test_dataloader = load_data(config)\n",
        "    if continue_train:\n",
        "        vgg, model = get_networks(config, load_checkpoint=True)\n",
        "    else:\n",
        "        vgg, model = get_networks(config)\n",
        "\n",
        "    # Criteria And Optimizers\n",
        "    criterion = MseDirectionLoss(lamda)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    if continue_train:\n",
        "        optimizer.load_state_dict(\n",
        "            torch.load('{}Opt_{}_epoch_{}.pth'.format(checkpoint_path, normal_class, last_checkpoint)))\n",
        "\n",
        "    losses = []\n",
        "    roc_aucs = []\n",
        "    if continue_train:\n",
        "        with open('{}Auc_{}_epoch_{}.pickle'.format(checkpoint_path, normal_class, last_checkpoint), 'rb') as f:\n",
        "            roc_aucs = pickle.load(f)\n",
        "\n",
        "    for epoch in range(num_epochs + 1):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for data in train_dataloader:\n",
        "            X = data[0]\n",
        "            if X.shape[1] == 1:\n",
        "                X = X.repeat(1, 3, 1, 1)\n",
        "            X = Variable(X).cuda()\n",
        "\n",
        "            output_pred = model.forward(X)\n",
        "            output_real = vgg(X)\n",
        "\n",
        "            total_loss = criterion(output_pred, output_real)\n",
        "\n",
        "            # Add loss to the list\n",
        "            epoch_loss += total_loss.item()\n",
        "            losses.append(total_loss.item())\n",
        "\n",
        "            # Clear the previous gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Compute gradients\n",
        "            total_loss.backward()\n",
        "            # Adjust weights\n",
        "            optimizer.step()\n",
        "\n",
        "        print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, epoch_loss))\n",
        "        if epoch % 10 == 0:\n",
        "            roc_auc = detection_test(model, vgg, test_dataloader, config)\n",
        "            roc_aucs.append(roc_auc)\n",
        "            print(\"RocAUC at epoch {}:\".format(epoch), roc_auc)\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            torch.save(model.state_dict(),\n",
        "                       '{}Cloner_{}_epoch_{}.pth'.format(checkpoint_path, normal_class, epoch))\n",
        "            torch.save(optimizer.state_dict(),\n",
        "                       '{}Opt_{}_epoch_{}.pth'.format(checkpoint_path, normal_class, epoch))\n",
        "            with open('{}Auc_{}_epoch_{}.pickle'.format(checkpoint_path, normal_class, epoch),\n",
        "                      'wb') as f:\n",
        "                pickle.dump(roc_aucs, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2Tft4BfIh3o",
        "outputId": "3633bf32-95a4-44f6-bbc1-ad7a77c8a609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./Dataset/CIFAR10/train/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:10<00:00, 16047897.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./Dataset/CIFAR10/train/cifar-10-python.tar.gz to ./Dataset/CIFAR10/train\n",
            "Cifar10 DataLoader Called...\n",
            "All Train Data:  (50000, 32, 32, 3)\n",
            "Normal Train Data:  (5000, 32, 32, 3)\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./Dataset/CIFAR10/test/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:10<00:00, 15535851.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./Dataset/CIFAR10/test/cifar-10-python.tar.gz to ./Dataset/CIFAR10/test\n",
            "Test Train Data: (10000, 32, 32, 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:12<00:00, 43.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer : 0 Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 1 BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer : 2 ReLU(inplace=True)\n",
            "layer : 3 Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 4 ReLU(inplace=True)\n",
            "layer : 5 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "layer : 6 Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 7 BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer : 8 ReLU(inplace=True)\n",
            "layer : 9 Conv2d(16, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 10 ReLU(inplace=True)\n",
            "layer : 11 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "layer : 12 Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 13 BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer : 14 ReLU(inplace=True)\n",
            "layer : 15 Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 16 BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer : 17 ReLU(inplace=True)\n",
            "layer : 18 Conv2d(16, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 19 ReLU(inplace=True)\n",
            "layer : 20 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "layer : 21 Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 22 BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer : 23 ReLU(inplace=True)\n",
            "layer : 24 Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 25 BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer : 26 ReLU(inplace=True)\n",
            "layer : 27 Conv2d(16, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 28 ReLU(inplace=True)\n",
            "layer : 29 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "layer : 30 Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 31 BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer : 32 ReLU(inplace=True)\n",
            "layer : 33 Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 34 BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer : 35 ReLU(inplace=True)\n",
            "layer : 36 Conv2d(16, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 37 ReLU(inplace=True)\n",
            "layer : 38 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "epoch [1/1], loss:54.0776\n",
            "RocAUC at epoch 0: 0.77\n",
            "epoch [2/1], loss:53.9909\n"
          ]
        }
      ],
      "source": [
        "train(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PxELHoRKKpj",
        "outputId": "9bb5ee05-f95b-461e-c0a7-12489e5000e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer : 0 Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 1 BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer : 2 ReLU(inplace=True)\n",
            "layer : 3 Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 4 ReLU(inplace=True)\n",
            "layer : 5 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "layer : 6 Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 7 BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer : 8 ReLU(inplace=True)\n",
            "layer : 9 Conv2d(16, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 10 ReLU(inplace=True)\n",
            "layer : 11 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "layer : 12 Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 13 BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer : 14 ReLU(inplace=True)\n",
            "layer : 15 Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 16 BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer : 17 ReLU(inplace=True)\n",
            "layer : 18 Conv2d(16, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 19 ReLU(inplace=True)\n",
            "layer : 20 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "layer : 21 Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 22 BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer : 23 ReLU(inplace=True)\n",
            "layer : 24 Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 25 BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer : 26 ReLU(inplace=True)\n",
            "layer : 27 Conv2d(16, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 28 ReLU(inplace=True)\n",
            "layer : 29 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "layer : 30 Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 31 BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer : 32 ReLU(inplace=True)\n",
            "layer : 33 Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 34 BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer : 35 ReLU(inplace=True)\n",
            "layer : 36 Conv2d(16, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer : 37 ReLU(inplace=True)\n",
            "layer : 38 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "Files already downloaded and verified\n",
            "Cifar10 DataLoader Called...\n",
            "All Train Data:  (50000, 32, 32, 3)\n",
            "Normal Train Data:  (5000, 32, 32, 3)\n",
            "Files already downloaded and verified\n",
            "Test Train Data: (10000, 32, 32, 3)\n",
            "RocAUC after 200 epoch: 0.7703\n"
          ]
        }
      ],
      "source": [
        "vgg, model = get_networks(config, load_checkpoint=True)\n",
        "\n",
        "# Detection test\n",
        "_, test_dataloader = load_data(config)\n",
        "roc_auc = detection_test(model=model, vgg=vgg, test_dataloader=test_dataloader, config=config)\n",
        "last_checkpoint = config['last_checkpoint']\n",
        "print(\"RocAUC after {} epoch:\".format(last_checkpoint), roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuo6h6MRRugT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}